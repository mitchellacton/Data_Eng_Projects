Spark/Scala Project
# Introduction
- A Big Data solution is needed to process large volumes of retail consumer data, to generate insights regarding sales
and customers
- The majority of this project was completed using Pyspark in Azure Databricks

# Databricks and Hadoop Implementation
- Using Azure Databricks, Pyspark, and a dataset of retail sales data, insights regarding monthly sales, monthly new
and returning customers, and classified customers into different groups.
- The notebook was run on a Databricks cluster consisting of one master and one worker node.


# Zeppelin and Hadoop Implementation
- The WDI dataset used in the Hadoop project was imported for this project, but this time Pyspark was used instead
of Hive
- The dataset was uploaded to HDFS, and was analyzed using the Pyspark interpreter within Zeppelin.
- Draw an architecture diagram

# Future Improvement
- Importing the data from a JDBC database instead of from a file
- taking advantage of Python's machine learning libraries for more advanced analysis
- Automate monthly data imports and reporting